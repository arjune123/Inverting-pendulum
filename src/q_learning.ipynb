{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import pendulum\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we set theta and dtheta = 0 and u = -5, we can get the next state using\n",
    "x = np.array([0,0])\n",
    "u = -5\n",
    "x_next = pendulum.get_next_state(x, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_theta = np.linspace(0, 2*np.pi, 50, endpoint=False)\n",
    "\n",
    "discretized_thetadot = np.linspace(-6, 6, 50)\n",
    "def get_cost(x,u):\n",
    "    theta = x[0]\n",
    "    theta_dot = x[1]\n",
    "    res = (theta-np.pi)**2+0.01*theta_dot**2+0.0001*u**2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_solver:\n",
    "    def __init__(self, costfn = get_cost, actions=None, max_iters=5000, sparse_loss=False):\n",
    "        if actions is None:\n",
    "            actions = [-5, 0, 5]\n",
    "        self.action_list = np.array(actions)\n",
    "        self.lr = 0.1\n",
    "        self.epslion = 0.1\n",
    "        self.step_in_iter = 100\n",
    "        self.alpha = 0.99\n",
    "        self.max_iters = max_iters\n",
    "        self.sparse_loss = sparse_loss\n",
    "        self.costfn = costfn\n",
    "        self.discretized_theta = discretized_theta\n",
    "        self.discretized_theta_dot = discretized_thetadot\n",
    "        self.space_shape = [len(discretized_theta),len(discretized_thetadot)]\n",
    "\n",
    "        self.num_states = 50*50\n",
    "        self.nu = 3\n",
    "        self.nq = 50\n",
    "        self.make_state_transfer_table()\n",
    "    def make_state_transfer_table(self):\n",
    "        next_state_index = np.empty([self.num_states, self.nu], dtype=np.int32)\n",
    "        for i in range(self.num_states):\n",
    "            for k in range(self.nu):\n",
    "                x_next = pendulum.get_next_state(self.get_states(i), self.action_list[k])\n",
    "                next_state_index[i, k] = self.get_index(x_next)\n",
    "\n",
    "        self.state_transfer_table = next_state_index #[250 3 2]\n",
    "\n",
    "\n",
    "    def get_index(self,x):\n",
    "        ind_q = np.argmin((x[0] - self.discretized_theta) ** 2)\n",
    "        ind_v = np.argmin((x[1] - self.discretized_theta_dot) ** 2)\n",
    "        return ind_q + ind_v * self.nq\n",
    "\n",
    "    def get_states(self, index):\n",
    "        iv, ix = np.divmod(index, self.nq)\n",
    "        return np.array([self.discretized_theta[ix], self.discretized_theta_dot[iv]])\n",
    "\n",
    "\n",
    "    def get_policy_and_value_function(self):\n",
    "        q = np.zeros([self.num_states, self.nu])\n",
    "        q_Last = np.zeros([self.num_states, self.nu])\n",
    "        for i in tqdm.tqdm(range(self.max_iters)):  #\n",
    "\n",
    "            # choose initial state x0\n",
    "            x_0 = np.array([0, 0])\n",
    "            x_index = self.get_index(x_0)\n",
    "            for j in range(self.step_in_iter):\n",
    "\n",
    "                if np.random.uniform(0, 1) > self.epslion:\n",
    "                    u_index = np.argmin(q[x_index, :])\n",
    "                else:\n",
    "                    u_index = np.random.randint(0, self.nu - 1)\n",
    "                # observe x_t+1\n",
    "                next_index = self.state_transfer_table[x_index, u_index]\n",
    "                # compute g(x_t,u(x_t))\n",
    "                x = self.get_states(x_index)\n",
    "                u = self.action_list[u_index]\n",
    "                # compute TDerror\n",
    "                TDerror = self.costfn(x, u) + self.alpha * min(q[next_index, :]) - q[\n",
    "                    x_index, u_index]\n",
    "                q[x_index, u_index] = q[x_index, u_index] + self.lr * TDerror\n",
    "                x_index = next_index\n",
    "\n",
    "            # we update the current Q function if there is any change otherwise we are done\n",
    "            if ((q_Last - q) ** 2 < 1e-5).all():\n",
    "                break\n",
    "            else:\n",
    "                q_Last = q.copy()\n",
    "\n",
    "        policy = np.zeros(self.space_shape)\n",
    "        value_function = np.zeros(self.space_shape)\n",
    "        for k in range(self.num_states):\n",
    "            iv, ix = np.divmod(k, self.nq)\n",
    "            policy[ix,iv] = self.action_list[np.argmin(q[k, :])]\n",
    "            value_function[ix,iv] = min(q[k, :])\n",
    "        return value_function,policy\n",
    "solver = Q_solver(get_cost,max_iters=50000)\n",
    "value,policy = solver.get_policy_and_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# here is some code to plot results, assuming a policy and a value function are given\n",
    "# this can be used to answer questions in both Part 1 and 2\n",
    "\n",
    "# we plot the value function\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.imshow(value.T, extent=[0., 2*np.pi, -6, 6], aspect='auto')\n",
    "plt.xlabel('Pendulum Angle')\n",
    "plt.ylabel('Velocity')\n",
    "plt.title('Value Function')\n",
    "\n",
    "# we plot the policy\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.imshow(policy.T, extent=[0., 2*np.pi, -6, 6], aspect='auto')\n",
    "plt.xlabel('Pendulum Angle')\n",
    "plt.ylabel('Velocity')\n",
    "plt.title('Policy')\n",
    "\n",
    "# now we simulate the dynamics for 100 time steps\n",
    "x0 = np.array([0.,0.])\n",
    "\n",
    "def controller(x):\n",
    "    theta = np.linspace(0, 2*np.pi, 50, endpoint=False)\n",
    "    dtheta = np.linspace(-6, 6, 50)\n",
    "    \n",
    "    th_index = np.argmin(np.abs(theta - x[0]))\n",
    "    dth_index = np.argmin(np.abs(dtheta - x[1]))\n",
    "    return policy[th_index, dth_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, x, u = pendulum.simulate(x0, controller, 30)\n",
    "\n",
    "# and plot the results\n",
    "time = np.linspace(0.,20., len(x[0,:]))\n",
    "plt.figure()\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(time,x[0,:])\n",
    "plt.ylabel('angle')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(time,x[1,:])\n",
    "plt.ylabel('velocity')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(time[:-1],u)\n",
    "plt.ylabel('control')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pendulum.animate_robot(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
